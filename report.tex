\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\geometry{
    margin=1in,
    top=10mm,
}

\title{Reinforcement Learning CW1}
\author{Taixi Pan}
\date{November 2022}

\begin{document}

    \maketitle


    \section{Question 1}\label{sec:question-1}

    \subsection{Part 1}\label{subsec:part-1}


    \section{Question 2}\label{sec:question-2}

    \subsection{Part 1}\label{subsec:part-12}
    I used an on-policy $\epsilon$-greedy first-visit MC iterative learning to control algorithm to solve the problem.
    $\epsilon$-greedy policy is used to allow for exploring the states, as the starting states are only part of the total states.
    The hyperparameter $\epsilon$ is set to 0.5 to allow for a certain degree of exploration.
    TODO $\epsilon$ is decreasing with number of episodes to satisfy the GLIE convergence condition.
    The total number of episodes is set to 1000.
    This decision is guided by plotting total non-discounted reward vs episodes TODO.
    First-visit MC is chosen for its effectiveness and simplicity.
    The assumption made about the environment is that the reward would be constant with respect to time.
    In other words, the environment would not change with time.

    \subsection{Part 2}\label{subsec:part-2}
    The policy visualization TODO

    \subsection{Part 3}\label{subsec:part-3}
    The randomness comes from the fact that actions are chosen from a set of possible actions, each assigned with some probability.
    The choice made every time can be different and random.
    To establish significance of the final learned policy, multiple runs are carried out and the distribution of final total non-discounted rewards is obtained.
    TODO From the summary statistics of different number of runs we can conclude that TODO replications would be sufficient.

    \subsection{Part 4}\label{subsec:part-4}
    blabla TODO

    \subsection{Part 5}\label{subsec:part-5}
    varying $\epsilon$
    varying $\alpha$
    TODO


    \section{Question 3}\label{sec:question-3}

    \subsection{Part 1}\label{subsec:part-13}
    I used an on-policy TD learning to control algorithm to solve the problem.
    The learning rate $\alpha$ is set to 1e-2 and decreasing with respect to t to allow for convergence.
    The choice of actions follows $\epsilon$-greedy with a decreasing $\epsilon$ from initial value 0.5 to satisfy GLIE.
    The assumption made is that the environment is stationary.

\end{document}
